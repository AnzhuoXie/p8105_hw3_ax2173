---
title: "p8015_hw3_ax2173"
output: github_document
---

```{r}  
library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = '90%'
)

theme_set(theme_classic() + theme(legend.position = 'bottom'))

options(
  ggplot2.continous.colour = 'viridis_d',
  ggplot2.continous.fill = 'viridis_d'
)

scalr_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Import the required data.

```{r}
library(p8105.datasets)
data("instacart")
```


## Problem 2

Import, clean and manipulate data.

```{r}
acc_df = 
  read_csv("./Data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    weekday_or_weekend = ifelse((day == 'Saturday' | day == "Sunday"), 'Weekend', 'Weekday')
    ) %>% 
  pivot_longer(
    activity_1 : activity_1440,
    names_to = 'activity_num',
    values_to = 'activity_counts'
  ) %>% 
  select(week, day_id, day, weekday_or_weekend, everything())
```

* There are some descriptions:
  * This dataset contains these variables: `r colnames(acc_df)`
  * There are totally `r nrow(acc_df)` observations
  * There are totally `r ncol(acc_df)` variables

Aggregate across minutes to create a total activity variable for each day, and create a table to show these totals. 

* As for any trends apparent, I am not sure, but I guess the day activity is relatively lower in weekends than in weekdays.

```{r}
acc_df %>% 
  group_by(week, day_id, day) %>% 
  summarize(day_activity = sum(activity_counts)) %>% 
  knitr::kable(digits = 2)
```

Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week.

```{r fig.width=6, fig.height=3}
acc_df %>% 
  group_by(week, day_id, day) %>% 
  summarize(day_activity = sum(activity_counts)) %>% 
  ggplot(aes(x = day_id, y = day_activity, color = week)) +
  geom_point() +
  geom_line() +
  labs(
    title = 'The Daily Activity of This 63-year-old Man',
    x = 'the Day ID',
    y = 'the Day Activity',
    caption = 'the day activity is aggregated across minutes'
  )
```

* About some patterns or conclusions I can make based on this graph.
  * A huge fluctuation can be found during the whole process
  * The highest data was in the Monday of week 3, reaching 685920.00
  * The lowest data appeared both in Saturday in week 4 and Saturday in week 5, with daily activity data of just 1440.00

## Problem 3

Download the needed data.

```{r}
library(p8105.datasets)
data("ny_noaa")
```

* There are some descriptions about the data:
  * The size of this dataset is `r nrow(ny_noaa)` x `r ncol(ny_noaa)`
  * This dataset had these variables: `r colnames(ny_noaa)`
  * Almost all the data in this dataset are missing, in other words, the extend of missing value is huge (I guess).
  
Next, do the data cleaning and create separate variables for year, month and day, and make sure that temperature, precipitation and snow fall are given in reasonable units.
  
```{r}
ny_noaa = 
  ny_noaa %>% 
  janitor::clean_names() %>% 
  mutate(
    tmax = as.double(tmax),
    tmax = tmax/10,
    tmin = as.double(tmin),
    tmin = tmin/10,
    prcp = prcp/10,
    snow = snow/100
    ) %>% 
  separate(date, into = c('year', 'month',  'day'), sep = '-')
```

Calculate the most commonly observed values about snowfall. 
  * From the density plot, we can know that the most commonly observed values about snowfall is 0.

```{r fig.width=9, fig.height=6}
ny_noaa %>% 
  group_by(snow) %>% 
    summarize(snowfall_count = n()) %>% 
  ggplot(aes(x = snow)) +
  geom_density() +
    labs(
    title = 'The Plot of Snowfall Data',
    x = 'Snowfall(mm)',
    y = 'Snowfall Count',
    caption = 'Data is from noaa package'
  )
```

Make a two-panel plot showing the average max temperature in January and in July in each station across years.
  * The median of average of max temperature in January is around 0 C, and that of July is around 26C.
  * The data in January is spread more wider than that of July.
  * Both of these data of two months have outliers: four for January and three for July.

```{r fig.width=7, fig.height=5}
ny_noaa %>% 
  filter(month == '01' | month == '07')  %>% 
  group_by(id, month) %>% 
    summarize(average_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = month, y = average_tmax)) +
  geom_boxplot() +
  labs(
    title = 'The Plot of Average Max Temperature in January and February',
    x = 'Month',
    y = 'Average Max Temperature(C)',
    caption = 'Data is from noaa package')
```

Make a two-panel plot showing tmax vs tmin for the full dataset
  * The median of max temperature is around 15 C, and the median of min temperature is around 3 C.
  * The data in tmin is spread more wider than that of tmax.
  * Both of these data many have outliers.

```{r fig.width=7, fig.height=5}
ny_noaa %>% 
  pivot_longer(
    tmax : tmin,
    names_to = 'temperature_category',
    values_to = 'temperature'
  ) %>% 
  ggplot(aes(x = temperature_category, y = temperature)) +
  geom_boxplot()  +
  labs(
    title = 'The Plot of Tmax vs Tmin for the Full Dataset',
    x = 'Temperature Category',
    y = 'Temperature(C)',
    caption = 'Data is from noaa package')
```

Make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r fig.width=9, fig.height=6}
ny_noaa %>% 
  filter(snow > 0 & snow <100) %>% 
  group_by(year) %>% 
    summarize(snowfall = sum(snow, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = snowfall)) + 
  geom_point() +
  labs(
    title = 'The Distribution of Snowfall',
    x = 'Year',
    y = 'Snowfall(mm)',
    caption = 'Data is from noaa package'
  )
```
